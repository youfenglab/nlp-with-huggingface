[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "nlp-with-huggingface",
    "section": "",
    "text": "This file will become your README and also the index of your documentation."
  },
  {
    "objectID": "hf_datasets.html",
    "href": "hf_datasets.html",
    "title": "Understanding Huggingface datasets",
    "section": "",
    "text": "Load a dataset from huggingface\n1.Load the whole dataset\n\nemotions = load_dataset('emotion')\nemotions\n\nUsing custom data configuration default\nReusing dataset emotion (/Users/youfeng/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705)\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 16000\n    })\n    validation: Dataset({\n        features: ['text', 'label'],\n        num_rows: 2000\n    })\n    test: Dataset({\n        features: ['text', 'label'],\n        num_rows: 2000\n    })\n})\n\n\n2.Load part of the dataset\n\nemotions = load_dataset('emotion', split='train')\nemotions\n\nUsing custom data configuration default\nReusing dataset emotion (/Users/youfeng/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705)\n\n\nDataset({\n    features: ['text', 'label'],\n    num_rows: 16000\n})\n\n\n\nemotions = load_dataset('emotion', split=['validation', 'test'])\nemotions\n\nUsing custom data configuration default\nReusing dataset emotion (/Users/youfeng/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705)\n\n\n\n\n\n[Dataset({\n     features: ['text', 'label'],\n     num_rows: 2000\n }),\n Dataset({\n     features: ['text', 'label'],\n     num_rows: 2000\n })]\n\n\n\n\nLoad a csv file from remote or local\n\ntrn_csv = 'https://raw.githubusercontent.com/youfenglab/nlp-with-huggingface/master/data/train.csv'\ntst_csv = 'https://raw.githubusercontent.com/youfenglab/nlp-with-huggingface/master/data/train.csv'\n\n\ntrn_df = pd.read_csv(trn_csv)\nlabel_cols = trn_df.columns[2:]\nlabel_cols\n\nIndex(['cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar',\n       'conventions'],\n      dtype='object')\n\n\n\ntrn_ds = load_dataset(\"csv\", data_files=trn_csv) # or several files []\ntrn_ds\n\nUsing custom data configuration default-8f5d5f2de1b27b24\nReusing dataset csv (/Users/youfeng/.cache/huggingface/datasets/csv/default-8f5d5f2de1b27b24/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a)\n\n\n\n\n\nDatasetDict({\n    train: Dataset({\n        features: ['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n        num_rows: 3911\n    })\n})\n\n\n\n\n\n\n\n\nTip: Load several files\n\n\n\ndata_files= [file1, file2 ...], but the columns of the files should be the same, otherwise you will get error.\n\n\nIf you want to split the training set into train and validation parts, you can do it as below\n\nds = trn_ds['train'].train_test_split(test_size=0.2) # Here we use 20% samples as validation part\nds\n\nDatasetDict({\n    train: Dataset({\n        features: ['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n        num_rows: 3128\n    })\n    test: Dataset({\n        features: ['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n        num_rows: 783\n    })\n})\n\n\nIf you want to use param “stratify” like scikit-learn, you can do it as below\n\n\n\n\n\n\nWarning\n\n\n\nNote: we need covert the column into ClassLabel column, since in this case the label columns are Value now. Otherwise, we will get an error like this:\nValueError: Stratifying by column is only supported for ClassLabel column, and column cohesion is Value.\n\n\nLet say we want use cohesion column.\n\ntrn_ds\n\nDatasetDict({\n    train: Dataset({\n        features: ['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n        num_rows: 3911\n    })\n})\n\n\n\nds = trn_ds.class_encode_column('cohesion')\nds['train'].features\n\n\n\n\n\n\n\n\n\n\n{'text_id': Value(dtype='string', id=None),\n 'full_text': Value(dtype='string', id=None),\n 'cohesion': ClassLabel(num_classes=9, names=['1.0', '1.5', '2.0', '2.5', '3.0', '3.5', '4.0', '4.5', '5.0'], id=None),\n 'syntax': Value(dtype='float64', id=None),\n 'vocabulary': Value(dtype='float64', id=None),\n 'phraseology': Value(dtype='float64', id=None),\n 'grammar': Value(dtype='float64', id=None),\n 'conventions': Value(dtype='float64', id=None)}\n\n\n\nds = ds['train'].train_test_split(test_size=0.2, stratify_by_column=\"cohesion\")\nds\n\nDatasetDict({\n    train: Dataset({\n        features: ['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n        num_rows: 3128\n    })\n    test: Dataset({\n        features: ['text_id', 'full_text', 'cohesion', 'syntax', 'vocabulary', 'phraseology', 'grammar', 'conventions'],\n        num_rows: 783\n    })\n})\n\n\n\n\nHow does a dataset be structured?"
  }
]